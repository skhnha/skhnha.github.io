<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<!-- # reference : https://github.com/kpertsch/kpertsch.github.io/blob/master/index.html -->
<head>
	<!-- Global Site Tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-112301535-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-112301535-1');
	</script>
	  <meta name=viewport content=“width=800”>
	  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
	  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    li:not(:last-child) {
        margin-bottom: -10px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }

    img {
      border-radius: 15px;
    }
  </style>
  <link rel="icon" type="image/png" href="images/myprofile.png">
  <title>Seokhyeon Ha</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
        <tr>
          <td>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                <td width="80%" valign="middle">
                  <p align="center">
                    <name>Seokhyeon Ha</name>
                  </p>
                  <p>I am interested in artificial intelligence and machine learning. I am currently focusing on research in large language models.
                  </p>
                  <p>                            
                    <ul>
                      <li>Currently working as a Staff Engineer at Samsung Electronics (2024-)</li><br>
                      <li>PhD degree from Department of Eletrical and Computer Engineering, Seoul National University (2017-2024), advised by <a target="_blank" href="https://cml.snu.ac.kr/?page_id=5948">Prof. Jungwoo Lee</a></li><br>
                      <li>Bachelor's degree from Department of Eletrical and Computer Engineering, Seoul National University (2013-2017)</li><br>
                    </ul>                    
                  </p>
                  <p align=center>
                    <a href="mailto:hash1108@cml.snu.ac.kr">Email</a> &nbsp/&nbsp
                    <a target="_blank" href="https://scholar.google.com/citations?user=PL-b34sAAAAJ">Google Scholar</a> &nbsp/&nbsp
                    <a target="_blank" href="https://github.com/skhnha">Github</a> &nbsp/&nbsp
                    <a target="_blank" href="https://www.linkedin.com/in/sh-ha-923616300/"> LinkedIn </a>
                  </p>
                </td>
                <td width="20%">
                  <img src="images/myprofile.png" width="200" height="200">
                </td>
              </tr>
            </table>
    
            <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                <td width="100%" valign="middle">
                  <heading>News</heading>
                  <p>
                    <ul>
                    <li> [Oct 2022] Our work on  <a target="_blank" href="https://kpertsch.github.io/star">cross-domain imitation learning</a> got accepted to CoRL'22! </li> <br>
                    <li> [Mar 2022] Two papers accepted to ICLR 2022: <a target="_blank" href="https://clvrai.com/tarp">Task-Induced Representation Learning</a> and <a target="_blank" href="https://clvrai.com/simpl">Skill-based Meta-Reinforcement Learning</a>! </li> <br>
                     <li> [Sept 2021] Our <a target="_blank" href="https://arxiv.org/abs/2107.10253">SkiLD paper</a> will be presented at CoRL 2021! </li> <br>
                    <li> [Jul 2021] New <a target="_blank" href="https://arxiv.org/abs/2107.10253">preprint</a> on skill-based learning with demonstrations! </li> <br>
                    <li> [Jun 2021] I presented our work on skill-based reinforcement & imitation learning in the <a target="_blank" href=https://www.seas.upenn.edu/~dineshj/pal/index.html">PAL Lab</a> at UPenn and in the <a target="_blank" href=http://svl.stanford.edu/">Stanford Vision & Learning Lab</a>. Check the <a target="_blank" href="https://drive.google.com/file/d/14xn9ojYfv8rSxVf5fPTixnkTpJRWUKKg/view?usp=sharing">Slides here</a>!</li><br>
                    <li> [Dec 2020] Received the CoRL 2020 <span style="color: #ff0000">Best Paper Presentation Award</span> for our <a target="_blank" href="https://arxiv.org/abs/2010.11944">SPiRL</a> paper, check out the <a target="_blank" href="https://youtu.be/kZOcqFRj5NE?t=5119">talk recording</a>!</li><br>
                      <li> [Nov 2020] <a target="_blank" href="https://arxiv.org/abs/2010.11944">SPiRL</a> will be presented as a <span style="color: #ff0000">plenary talk</span> at CoRL & won the <span style="color: #ff0000">best paper runner-up award</span> at the robot learning workshop @ NeurIPS!</li><br>
                    <li> [Oct 2020] Two papers accepted to CoRL 2020 (<a target="_blank" href="https://arxiv.org/abs/2010.11944">SPiRL</a> and <a target="_blank" href="https://arxiv.org/abs/2010.11940">MoPA-RL</a>)! </li> <br> -->
                    <!-- <li> [Sept 2020] Our hierarchical prediction and planning paper was accepted to NeurIPS2020!</li> <br> -->
                    <!-- <li> [Jun 2020] New <a target="_blank" href="https://arxiv.org/abs/2006.13205">preprint</a> on long-horizon visual planning using hierarchical prediction! </li> <br> -->
                      <!-- <li> [Apr 2020] Our work on keyframe-based prediction will be presented at L4DC'20!</li> <br> -->
                    <!-- <li> [Apr 2019] New <a target="_blank" href="https://arxiv.org/abs/1904.05869">preprint</a> on keyframe-based video prediction! </li> <br> -->
                    <!-- <li> [Apr 2019] Our work on discovering an agent's action space got accepted to ICLR19! </li> <br> -->
                    <!--<li> [Dec 2018] I presented our work on unsupervised learning of agent's action spaces at the <a target="_blank" href="https://sites.google.com/view/infer2control-nips2018">Infer2Control workshop</a> at NeurIPS 2018 in Montreal. </li> <br> -->
                    <!-- <br> -->
                      <!-- <li> [Aug 2017] Starting my one year Fulbright research stay in <a target="_blank" href="http://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a> group at UPenn. </li> <br> -->
                    <!-- </ul>  
                  </p>
                </td>
              </tr>
              </table> -->
    
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                <td width="100%" valign="middle">
                  <heading>Research</heading>
                </td>
              </tr>
            </table>
    
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    

            <tr  onmouseout="daft_start()" onmouseover="daft_start()">
              <td width="25%">
                <div class="one">
                <div class="two" id = 'daft_image'><img src='images/daft.png' width="160" height="160"></div>
                <img src='images/daft.png' width="160" height="160" style="z-index:-1">
                </div>
                <script type="text/javascript">
                function octo_start() {
                document.getElementById('daft_image').style.opacity = "1";
                }
                function octo_stop() {
                document.getElementById('daft_image').style.opacity = "0";
                }
                // spirl_stop()
                </script>
              </td>
              <td width="75%" valign="top">
              <p>
              <p>                
                <papertitle>Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability</papertitle>                
                <br>
                <strong>Seokhyeon Ha</strong>, Sunbeom Jung, Jungwoo Lee<br>
                <i>AAAI</i>, 2024<br>
                <a target="_blank" href ="https://arxiv.org/abs/2308.07728">paper</a>  / 
                <a target="_blank" href ="https://github.com/skhnha/DAFT">code</a>
                <br>
              </p>
              <p>We propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning. Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning. Additionally, we introduce the integration of linear probing and fine-tuning to optimize the head layer with gradual adaptation of the feature extractor.</p>              
              <p>
              </p>
              </td>
            </tr>


            <tr  onmouseout="mina_start()" onmouseover="mina_start()">
              <td width="25%">
                <div class="one">
                <div class="two" id = 'mina_image'><img src='images/mina.jpg' width="160" height="160"></div>
                <img src='images/mina.jpg' width="160" height="160" style="z-index:-1">
                </div>
                <script type="text/javascript">
                function octo_start() {
                document.getElementById('mina_image').style.opacity = "1";
                }
                function octo_stop() {
                document.getElementById('mina_image').style.opacity = "0";
                }
                // spirl_stop()
                </script>
              </td>
              <td width="75%" valign="top">
              <p>
              <p>                
                <papertitle>MINA: Multi-Input Network Augmentation for Enhancing Tiny Deep Learning</papertitle>                
                <br>
                <strong>Seokhyeon Ha</strong>, Yeongmo Kim, Jungwoo Lee<br>
                <i>IEEE ACCESS</i>, 2023<br>
                <a target="_blank" href ="https://ieeexplore.ieee.org/document/10262264">paper</a> 
                <br>
              </p>
              <p>We propose a new method called Multi-Input Network Augmentation (MINA). MINA converts tiny neural networks into a multi-input configuration, allowing only the augmented model to receive more diverse inputs during training. Additionally, tiny neural network can be converted back into their original single-input configuration after training.</p>              
              <p>
              </p>
              </td>
            </tr>


            <tr  onmouseout="meta_ens_start()" onmouseover="meta_ens_start()">
              <td width="25%">
                <div class="one">
                <div class="two" id = 'meta_ens_image'><img src='images/meta_ens.png' width="160" height="160"></div>
                <img src='images/meta_ens.png' width="160" height="160" style="z-index:-1">
                </div>
                <script type="text/javascript">
                function octo_start() {
                document.getElementById('meta_ens_image').style.opacity = "1";
                }
                function octo_stop() {
                document.getElementById('meta_ens_image').style.opacity = "0";
                }
                // spirl_stop()
                </script>
              </td>
              <td width="75%" valign="top">
              <p>
              <p>                
                <papertitle>Meta-ensemble learning with a multi-headed model for few-shot problems</papertitle>                
                <br>
                <strong>Seokhyeon Ha</strong>, Youngseok Yoon, Jungwoo Lee<br>
                <i>ICT Express</i>, 2023<br>
                <a target="_blank" href ="https://www.sciencedirect.com/science/article/pii/S2405959522001345">paper</a> 
                <br>
              </p>
              <p>We propose a novel meta-ensemble learning approach based on a recent ensemble method: a multi-input multi-output (MIMO) configuration. Our approach is simply applied to existing meta-learning algorithms. Multiple subnetworks in a single model simultaneously learn multiple episodes and ensemble the predictions, leveraging the model capacity.</p>              
              <p>
              </p>
              </td>
            </tr>

            
            <tr  onmouseout="de_darts_start()" onmouseover="de_darts_start()">
              <td width="25%">
                <div class="one">
                <div class="two" id = 'de_darts_image'><img src='images/de_darts.png' width="160" height="160"></div>
                <img src='images/de_darts.png' width="160" height="160" style="z-index:-1">
                </div>
                <script type="text/javascript">
                function octo_start() {
                document.getElementById('de_darts_image').style.opacity = "1";
                }
                function octo_stop() {
                document.getElementById('de_darts_image').style.opacity = "0";
                }
                // spirl_stop()
                </script>
              </td>
              <td width="75%" valign="top">
              <p>
              <p>                
                <papertitle>DE-DARTS: Neural architecture search with dynamic exploration</papertitle>                
                <br>
                Jiwoo Mun, <strong>Seokhyeon Ha</strong>, Jungwoo Lee<br>
                <i>ICT Express</i>, 2023<br>
                <a target="_blank" href ="https://www.sciencedirect.com/science/article/pii/S2405959522000613">paper</a> 
                <br>
              </p>
              <p>To overcome the bias of the gradient-based search, we make the architecture dynamic during the search. This simple technique allows the gradient-based search to have an exploration effect. For effective exploration, we propose Dynamic Attention Networks (DANs) which change the neural architecture based on the input.</p>              
              <p>
              </p>
              </td>
            </tr>

            
            <tr  onmouseout="radar_rnn_start()" onmouseover="radar_rnn_start()">
              <td width="25%">
                <div class="one">
                <div class="two" id = 'radar_rnn_image'><img src='images/radar_rnn.png' width="160" height="160"></div>
                <img src='images/radar_rnn.png' width="160" height="160" style="z-index:-1">
                </div>
                <script type="text/javascript">
                function octo_start() {
                document.getElementById('radar_rnn_image').style.opacity = "1";
                }
                function octo_stop() {
                document.getElementById('radar_rnn_image').style.opacity = "0";
                }
                // spirl_stop()
                </script>
              </td>
              <td width="75%" valign="top">
              <p>
              <p>                
                <papertitle>Automotive radar signal interference mitigation using RNN with self attention</papertitle>                
                <br>
                Jiwoo Mun, <strong>Seokhyeon Ha</strong>, Jungwoo Lee<br>
                <i>ICASSP</i>, 2020<br>
                <a target="_blank" href ="https://www.sciencedirect.com/science/article/pii/S2405959522000613">paper</a> 
                <br>
              </p>
              <p>We propose a new method using deep learning. We improve the performance of the existing deep learning algorithm using attention mechanism. We applied our algorithm to the OFDM radar environment as well as the existing frequency modulated continuous wave(FMCW) radar.</p>              
              <p>
              </p>
              </td>
            </tr>


</body>
</html>