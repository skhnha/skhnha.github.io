<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
	<!-- Global Site Tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-112301535-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-112301535-1');
	</script>
	  <meta name=viewport content=“width=800”>
	  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
	  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    li:not(:last-child) {
        margin-bottom: -10px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }

    img {
      border-radius: 15px;
    }
  </style>
  <link rel="icon" type="image/jpg" href="images/myprofile.jpg">
  <title>Seokhyeon Ha</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
        <tr>
          <td>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                <td width="67%" valign="middle">
                  <p align="center">
                    <name><strong>Seok Hyeon Ha</strong></name>
                  </p>
                  <p>I am a postdoc at UC Berkeley and Stanford University, where I work with <a target="_blank" href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a> and <a target="_blank" href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a> on training robot foundation models.</p>
    
                  <p>I completed my PhD at the University of Southern California (USC), working with <a target="_blank" href="http://www-bcf.usc.edu/~limjj/">Joseph Lim</a>. During my PhD, I was fortunate to intern at Meta AI and spend time as a student researcher at Google Brain with <a target="_blank" href="https://karolhausman.github.io/">Karol Hausman</a>. Before my PhD, I spent one year as a Fulbright Scholar at the University of Pennsylvania, working with <a target="_blank" href="http://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>.
                  </p>
                  <p align=center>
                    <a href="mailto:aoxm1231@gmail.com">Email</a> &nbsp/&nbsp
                    <a target="_blank" href="https://scholar.google.com/citations?view_op=list_works&hl=en&user=3oe0I0QAAAAJ">Google Scholar</a> &nbsp/&nbsp
                    <a target="_blank" href="https://scholar.google.com/citations?view_op=list_works&hl=en&user=3oe0I0QAAAAJ">Github</a> &nbsp/&nbsp
                    <a target="_blank" href="https://www.linkedin.com/in/karlpertsch/"> LinkedIn </a>
                  </p>
                </td>
                <td width="25%">
                  <img src="images/myprofile.jpg" width="250" height="250">
                </td>
              </tr>
            </table>
    
            <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                <td width="100%" valign="middle">
                  <heading>News</heading>
                  <p>
                    <ul>
                    <li> [Oct 2022] Our work on  <a target="_blank" href="https://kpertsch.github.io/star">cross-domain imitation learning</a> got accepted to CoRL'22! </li> <br>
                    <li> [Mar 2022] Two papers accepted to ICLR 2022: <a target="_blank" href="https://clvrai.com/tarp">Task-Induced Representation Learning</a> and <a target="_blank" href="https://clvrai.com/simpl">Skill-based Meta-Reinforcement Learning</a>! </li> <br>
                     <li> [Sept 2021] Our <a target="_blank" href="https://arxiv.org/abs/2107.10253">SkiLD paper</a> will be presented at CoRL 2021! </li> <br>
                    <li> [Jul 2021] New <a target="_blank" href="https://arxiv.org/abs/2107.10253">preprint</a> on skill-based learning with demonstrations! </li> <br>
                    <li> [Jun 2021] I presented our work on skill-based reinforcement & imitation learning in the <a target="_blank" href=https://www.seas.upenn.edu/~dineshj/pal/index.html">PAL Lab</a> at UPenn and in the <a target="_blank" href=http://svl.stanford.edu/">Stanford Vision & Learning Lab</a>. Check the <a target="_blank" href="https://drive.google.com/file/d/14xn9ojYfv8rSxVf5fPTixnkTpJRWUKKg/view?usp=sharing">Slides here</a>!</li><br>
                    <li> [Dec 2020] Received the CoRL 2020 <span style="color: #ff0000">Best Paper Presentation Award</span> for our <a target="_blank" href="https://arxiv.org/abs/2010.11944">SPiRL</a> paper, check out the <a target="_blank" href="https://youtu.be/kZOcqFRj5NE?t=5119">talk recording</a>!</li><br>
                      <li> [Nov 2020] <a target="_blank" href="https://arxiv.org/abs/2010.11944">SPiRL</a> will be presented as a <span style="color: #ff0000">plenary talk</span> at CoRL & won the <span style="color: #ff0000">best paper runner-up award</span> at the robot learning workshop @ NeurIPS!</li><br>
                    <li> [Oct 2020] Two papers accepted to CoRL 2020 (<a target="_blank" href="https://arxiv.org/abs/2010.11944">SPiRL</a> and <a target="_blank" href="https://arxiv.org/abs/2010.11940">MoPA-RL</a>)! </li> <br> -->
                    <!-- <li> [Sept 2020] Our hierarchical prediction and planning paper was accepted to NeurIPS2020!</li> <br> -->
                    <!-- <li> [Jun 2020] New <a target="_blank" href="https://arxiv.org/abs/2006.13205">preprint</a> on long-horizon visual planning using hierarchical prediction! </li> <br> -->
                      <!-- <li> [Apr 2020] Our work on keyframe-based prediction will be presented at L4DC'20!</li> <br> -->
                    <!-- <li> [Apr 2019] New <a target="_blank" href="https://arxiv.org/abs/1904.05869">preprint</a> on keyframe-based video prediction! </li> <br> -->
                    <!-- <li> [Apr 2019] Our work on discovering an agent's action space got accepted to ICLR19! </li> <br> -->
                    <!--<li> [Dec 2018] I presented our work on unsupervised learning of agent's action spaces at the <a target="_blank" href="https://sites.google.com/view/infer2control-nips2018">Infer2Control workshop</a> at NeurIPS 2018 in Montreal. </li> <br> -->
                    <!-- <br> -->
                      <!-- <li> [Aug 2017] Starting my one year Fulbright research stay in <a target="_blank" href="http://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a> group at UPenn. </li> <br> -->
                    <!-- </ul>  
                  </p>
                </td>
              </tr>
              </table> -->
    
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                <td width="100%" valign="middle">
                  <heading>Research</heading>
                  <p>
                    I'm interested in machine learning, reinforcement learning and robotics. At the moment, I am working on training foundation models for robotics. Towards this goal, I am building diverse robot learning datasets and explore how we can train large policies on top of them.
                  </p>
                </td>
              </tr>
            </table>
    
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    
            <tr  onmouseout="droid_start()" onmouseover="droid_start()">
              <td width="25%">
                <div class="one">
                <div class="two" id = 'droid_image'><img src='resources/droid.gif' width="160" height="160"></div>
                <img src='resources/droid.jpeg' width="160" height="160" style="z-index:-1">
                </div>
                <script type="text/javascript">
                function octo_start() {
                document.getElementById('droid_image').style.opacity = "1";
                }
                function octo_stop() {
                document.getElementById('droid_image').style.opacity = "0";
                }
                // spirl_stop()
                </script>
              </td>
              <td width="75%" valign="top">
              <p>
              <p>
                <a target="_blank" href="https://droid-dataset.github.io/">
                <papertitle>DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset</papertitle>
                </a>
                <br>
                <a target="_blank" href="https://www.linkedin.com/in/alexander-khazatsky-b98841149">Alexander Khazatsky</a>*, <strong>Karl Pertsch</strong>*, <a target="_blank" href="https://suraj-nair-1.github.io/">Suraj Nair</a>, ..., <a target="_blank" href="https://www.linkedin.com/in/tkollar">Thomas Kollar</a>, <a target="_blank" href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>, <a target="_blank" href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a><br>
                <i>ArXiV</i>, 2024<br>
                <a target="_blank" href ="https://droid-dataset.github.io/">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/2403.12945">paper</a>  / 
                <a target="_blank" href ="https://gdroid-dataset.github.io/visualizer">dataset visualizer</a>
                <br>
              </p>
              <p> We introduce DROID, the most diverse robot manipulation dataset to date. It contains 76k demonstration trajectories or 350 hours of interaction data, collected across 564 scenes and 84 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance and improved generalization ability. We open source the full dataset, policy learning code, and a detailed guide for reproducing our robot hardware setup.</p>
              <p>
              </p>
              </td>
            </tr>
</body>
</html>